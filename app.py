import os
import streamlit as st  # <--  砖!
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# 专转
VECTOR_STORE_PATH = "./vector_store"
os.environ["NVIDIA_API_KEY"] = "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" #  砖驻转 

# -----------------------------------------------------------------
# 驻拽爪 注转 砖专砖专转 - 注 "专 "
#  拽  砖!
# Streamlit 专抓 转  住拽专驻 砖  专拽爪.
# @st.cache_resource 专 -Streamlit "转专抓 转 驻拽爪  专拽 驻注 转,
# 转砖专 转 转爪 砖 专".
#  注   注 砖  驻注.
# -----------------------------------------------------------------
@st.cache_resource
def load_rag_chain():
    print("注 转  -RAG... ( 拽专 专拽 驻注 转)")
    
    # 1. 注转 -LLM ( 砖 砖 NVIDIA)
    llm = ChatNVIDIA(model="meta/llama3-8b-instruct")
    
    # 2. 注转  -Embeddings (住驻专 拽)
    embeddings = HuggingFaceEmbeddings(
        model_name="paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    # 3. 注转 住 转 拽 住拽
    if not os.path.exists(VECTOR_STORE_PATH):
        #  砖 拽专转,  注爪专 转 驻拽爪   DB
        st.error(f"砖: 转拽转 住 转 '{VECTOR_STORE_PATH}'  爪.")
        st.stop()
        
    vectorstore = Chroma(
        persist_directory=VECTOR_STORE_PATH, 
        embedding_function=embeddings
    )

    # 4. 专转 -RAG ("住驻专" 驻专驻)
    retriever = vectorstore.as_retriever()

    prompt_template = """
转 注专 专住. 注 注 砖转 砖转砖  专拽 
转住住 注 拽砖专 (Context) :
<context>
{context}
</context>
砖: {input}
"""
    prompt = ChatPromptTemplate.from_template(prompt_template)

    # 5. 转 砖专砖专转 (Chain)
    combine_docs_chain = create_stuff_documents_chain(llm, prompt)
    retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)
    
    return retrieval_chain

# --- 驻拽爪转 main 砖 (住住转 Streamlit) ---
def main():
    # --- 专转 注 ---
    st.set_page_config(page_title="爪' 驻拽", layout="wide")
    st.title(" 爪' 驻拽 (住住 RAG)")

    # --- 注转 砖专砖专转 ---
    # 注 转 砖专砖专转 驻注 转 转 -cache
    try:
        retrieval_chain = load_rag_chain()
    except Exception as e:
        #  -API Key  , 专 砖 驻
        if "Authorization failed" in str(e):
            st.error("砖转 转专转 -NVIDIA.   砖-NVIDIA_API_KEY 砖 .")
            st.stop()
        else:
            st.error(f"专注 砖 注转 : {e}")
            st.stop()

    # ---  专 (住专转 爪') ---
    # Streamlit  专 砖转  专爪转. 
    # 'st.session_state'  "专" 砖砖专
    if "messages" not in st.session_state:
        st.session_state.messages = [] # 转 住专转 爪' 专拽

    # 爪转 注转 砖转 住专
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # --- 拽转 拽 砖转砖 ---
    # 'st.chat_input' 爪专 转转 拽住 转转转 住
    if prompt := st.chat_input("砖 转 砖 注 拽专住..."):
        # 1. 住祝 转 注转 砖转砖 住专 爪
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. 拽 转砖 -RAG chain
        with st.chat_message("assistant"):
            # 住祝 "住驻专"  砖 砖
            with st.spinner("砖..."):
                response = retrieval_chain.invoke({"input": prompt})
                answer = response["answer"]
                st.markdown(answer)
        
        # 3. 住祝 转 转砖转  住专
        st.session_state.messages.append({"role": "assistant", "content": answer})

# 驻注
if __name__ == "__main__":
    main()