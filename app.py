import os
import streamlit as st
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # <--  砖
from langchain.chains.combine_documents import create_stuff_documents_chain
# ---  砖  住专 ---
from langchain.chains import create_retrieval_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_core.messages import HumanMessage, AIMessage

# 专转
VECTOR_STORE_PATH = "./vector_store"
os.environ["NVIDIA_API_KEY"] = "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" #  砖驻转 

# -----------------------------------------------------------------
# 驻拽爪 注转 专 (LLM, Embeddings, VectorStore)
# -----------------------------------------------------------------
@st.cache_resource
def get_components():
    print("注 专... ( 拽专 专拽 驻注 转)")
    
    # 1. 注转 -LLM ( 砖)
    llm = ChatNVIDIA(model="meta/llama3-8b-instruct")
    
    # 2. 注转  -Embeddings (住驻专)
    embeddings = HuggingFaceEmbeddings(
        model_name="paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    # 3. 注转 住 转
    if not os.path.exists(VECTOR_STORE_PATH):
        st.error(f"砖: 转拽转 住 转 '{VECTOR_STORE_PATH}'  爪.")
        st.stop()
        
    vectorstore = Chroma(
        persist_directory=VECTOR_STORE_PATH, 
        embedding_function=embeddings
    )
    
    # 4. 专转 -Retriever (注 砖 专 专)
    retriever = vectorstore.as_retriever(
    search_type="mmr", 
    search_kwargs={"k": 8, "fetch_k": 20}
    )
    
    return llm, retriever

# -----------------------------------------------------------------
# 驻拽爪 爪专转 砖专砖专转 RAG (驻注 注 专)
# -----------------------------------------------------------------
def create_rag_chain(llm, retriever):
    
    # --- 驻专驻 1: 砖转 砖 ---
    # 驻专驻 砖 转 -LLM 拽转 转 住专 转 砖 砖, 
    # 爪专 砖 注爪转
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )
    
    # --- 砖专砖专转 1: -Retriever 砖注 "专" ---
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    
    # --- 驻专驻 2: 注转 转砖 ---
    #  驻专驻 拽专 砖, 专拽 注 转住驻转 专
    qa_system_prompt = (
        "转 注专 专住. 注 注 砖转 砖转砖  专拽 "
        "转住住 注 拽砖专 (Context) :\n\n"
        "<context>\n{context}\n</context>"
    )
    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    # --- 砖专砖专转 2: 爪专转 转砖 住 ---
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    
    # --- 砖专砖专转  ---
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    return rag_chain

# --- 驻拽爪转 main (住住转 Streamlit) ---
def main():
    st.set_page_config(page_title="爪' 驻拽", layout="wide")
    st.title(" 爪' 驻拽 (注 专)")

    # --- 注转 专 ---
    try:
        llm, retriever = get_components()
    except Exception as e:
        if "Authorization failed" in str(e):
            st.error("砖转 转专转 -NVIDIA.   砖-NVIDIA_API_KEY 砖 .")
            st.stop()
        else:
            st.error(f"专注 砖 注转 : {e}")
            st.stop()

    # --- 爪专转 砖专砖专转 ---
    rag_chain = create_rag_chain(llm, retriever)

    # ---  专 (住专转 爪') ---
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = [] # 转 住专转 爪' 专拽

    # 爪转 注转 砖转 住专
    for msg in st.session_state.chat_history:
        # 专转 拽 砖 LangChain 拽住 驻砖 注专 爪
        if isinstance(msg, HumanMessage):
            with st.chat_message("user"):
                st.markdown(msg.content)
        elif isinstance(msg, AIMessage):
            with st.chat_message("assistant"):
                st.markdown(msg.content)

    # --- 拽转 拽 砖转砖 ---
    if prompt := st.chat_input("砖 转 砖 注 拽专住..."):
        # 爪转 注转 砖转砖
        with st.chat_message("user"):
            st.markdown(prompt)

        # --- 拽转 转砖 -RAG chain (注 住专) ---
        with st.chat_message("assistant"):
            with st.spinner("砖..."):
                # 驻注  砖  转 住专 砖专砖专转
                response = rag_chain.invoke({
                    "input": prompt,
                    "chat_history": st.session_state.chat_history
                })
                answer = response["answer"]
                st.markdown(answer)
        
        # 注 住专 (注 拽  砖 LangChain)
        st.session_state.chat_history.append(HumanMessage(content=prompt))
        st.session_state.chat_history.append(AIMessage(content=answer))

# 驻注
if __name__ == "__main__":
    main()