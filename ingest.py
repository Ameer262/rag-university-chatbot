import os
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader, DirectoryLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ×”×’×“×¨×•×ª × ×ª×™×‘×™×
DATA_PATH = "data/"
VECTOR_STORE_PATH = "./vector_store"

def main():
    print(f"--- ××ª×—×™×œ ×ª×”×œ×™×š ×˜×¢×™× ×ª ×›×œ ×”××™×“×¢ ×- {DATA_PATH} (×›×•×œ×œ ×ª×ª×™-×ª×™×§×™×•×ª) ---")
    
    documents = []
    
    # 1. ×˜×¢×™× ×ª ×§×‘×¦×™ PDF
    print("ğŸ” ×¡×•×¨×§ ×§×‘×¦×™ PDF...")
    # recursive=True ×××¤×©×¨ ×œ×—×¤×© ×‘×ª×•×š ×ª×ª×™-×ª×™×§×™×•×ª
    pdf_loader = DirectoryLoader(DATA_PATH, glob="**/*.pdf", loader_cls=PyPDFLoader, recursive=True)
    try:
        pdf_docs = pdf_loader.load()
        print(f"   âœ… × ××¦××• {len(pdf_docs)} ×“×¤×™×/××¡××›×™× ××¡×•×’ PDF.")
        documents.extend(pdf_docs)
    except Exception as e:
        print(f"   âš ï¸ ×©×’×™××” ×‘×˜×¢×™× ×ª PDF (××•×œ×™ ××™×Ÿ ×§×‘×¦×™× ×›××œ×”?): {e}")

    # 2. ×˜×¢×™× ×ª ×§×‘×¦×™ Word (DOCX)
    print("ğŸ” ×¡×•×¨×§ ×§×‘×¦×™ Word...")
    docx_loader = DirectoryLoader(DATA_PATH, glob="**/*.docx", loader_cls=Docx2txtLoader, recursive=True)
    try:
        docx_docs = docx_loader.load()
        if docx_docs:
            print(f"   âœ… × ××¦××• {len(docx_docs)} ××¡××›×™× ××¡×•×’ Word.")
            documents.extend(docx_docs)
        else:
            print("   â„¹ï¸ ×œ× × ××¦××• ×§×‘×¦×™ Word.")
    except Exception as e:
        print(f"   âš ï¸ ×©×’×™××” ×‘×˜×¢×™× ×ª Word: {e}")

    # 3. ×˜×¢×™× ×ª ×§×‘×¦×™ TXT (×¢× ×ª×™×§×•×Ÿ ×§×¨×™×˜×™ ×œ×¢×‘×¨×™×ª!)
    print("ğŸ” ×¡×•×¨×§ ×§×‘×¦×™ ×˜×§×¡×˜ (TXT)...")
    # loader_kwargs={'encoding': 'utf-8'} ×¤×•×ª×¨ ××ª ×‘×¢×™×™×ª ×”×’'×™×‘×¨×™×© ×‘×•×•×™× ×“×•×¡
    txt_loader = DirectoryLoader(
        DATA_PATH, 
        glob="**/*.txt", 
        loader_cls=TextLoader, 
        recursive=True, 
        loader_kwargs={'encoding': 'utf-8'}
    )
    try:
        txt_docs = txt_loader.load()
        if txt_docs:
            print(f"   âœ… × ××¦××• {len(txt_docs)} ××¡××›×™× ××¡×•×’ Text.")
            documents.extend(txt_docs)
        else:
            print("   â„¹ï¸ ×œ× × ××¦××• ×§×‘×¦×™ ×˜×§×¡×˜.")
    except Exception as e:
        print(f"   âš ï¸ ×©×’×™××” ×‘×˜×¢×™× ×ª ×”×˜×§×¡×˜: {e}")

    # ×‘×“×™×§×” ×× ××¦×× ×• ××©×”×• ×‘×›×œ×œ
    if not documents:
        print("âŒ ×©×’×™××” ×§×¨×™×˜×™×ª: ×œ× × ××¦××• ×©×•× ×§×‘×¦×™× ×‘×ª×™×§×™×™×ª data ××• ×‘×ª×ª×™-×”×ª×™×§×™×•×ª ×©×œ×”.")
        return

    print(f"\nğŸ“š ×¡×”'×› ××¡××›×™× ×œ×¢×™×‘×•×“: {len(documents)}")

    # 4. ×—×™×ª×•×š ×”× ×ª×•× ×™×
    print("âœ‚ï¸ ×—×•×ª×š ××ª ×”××™×“×¢ ×œ×—×ª×™×›×•×ª ×§×˜× ×•×ª (Chunks)...")
    # × ×©××¨×™× ×¢× ×”×”×’×“×¨×” ×”×›×™×¨×•×¨×’×™×ª ×©×¢×‘×“×” ×˜×•×‘
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=350, chunk_overlap=75)
    splits = text_splitter.split_documents(documents)
    print(f"   × ×•×¦×¨×• {len(splits)} ×—×ª×™×›×•×ª ××™×“×¢.")

    # 5. ×™×¦×™×¨×ª Embeddings ×•×©××™×¨×”
    print("ğŸ§  ×˜×•×¢×Ÿ ××ª ××•×“×œ ×”-Embeddings (×¨×‘-×œ×©×•× ×™)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'}
    )

    print(f"ğŸ’¾ ×©×•××¨ ××ª ×”××™×“×¢ ×œ××¡×“ ×”× ×ª×•× ×™× ×‘- {VECTOR_STORE_PATH}...")
    vectorstore = Chroma.from_documents(
        documents=splits, 
        embedding=embeddings,
        persist_directory=VECTOR_STORE_PATH
    )
    
    print("\nâœ¨ ×ª×”×œ×™×š ×”×”×›× ×” ×”×¡×ª×™×™× ×‘×”×¦×œ×—×”! ×”×‘×•×˜ ××•×›×Ÿ.")

if __name__ == "__main__":
    main()